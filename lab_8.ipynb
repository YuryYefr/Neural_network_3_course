{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CNN-bi-LSTM for Speech Recognition\n",
    "\n",
    "This notebook implements a CNN-bi-LSTM neural network for speech recognition using the LJ-Speech dataset. The model converts speech to text using a combination of convolutional neural networks (CNN) and bidirectional long short-term memory (LSTM) networks.\n"
   ],
   "id": "37422d6a85c9228e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import wave\n",
    "from IPython.display import Audio\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "id": "d8a0fda34394d515",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "First, let's load the metadata and explore the dataset.\n"
   ],
   "id": "c72a7efcd89ebb89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to the dataset\n",
    "data_path = \"data/LJSpeech-1.1/\"\n",
    "wavs_path = os.path.join(data_path, \"wavs/\")\n",
    "metadata_path = os.path.join(data_path, \"metadata.csv\")\n",
    "\n",
    "# Load metadata\n",
    "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
    "metadata_df.columns = [\"file_id\", \"transcription\", \"normalized_transcription\"]\n",
    "metadata_df[\"wav_file\"] = metadata_df[\"file_id\"].apply(lambda x: os.path.join(wavs_path, f\"{x}.wav\"))\n",
    "\n",
    "# Display the first few rows\n",
    "metadata_df.head()\n"
   ],
   "id": "c248d9e6529d5907",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the number of samples\n",
    "print(f\"Total number of samples: {len(metadata_df)}\")\n",
    "\n",
    "# Check if all wav files exist\n",
    "missing_files = [f for f in metadata_df[\"wav_file\"] if not os.path.exists(f)]\n",
    "print(f\"Number of missing wav files: {len(missing_files)}\")\n",
    "\n",
    "# Get some statistics about the transcriptions\n",
    "metadata_df[\"transcription_length\"] = metadata_df[\"transcription\"].apply(len)\n",
    "print(f\"Average transcription length: {metadata_df['transcription_length'].mean():.2f} characters\")\n",
    "print(f\"Min transcription length: {metadata_df['transcription_length'].min()} characters\")\n",
    "print(f\"Max transcription length: {metadata_df['transcription_length'].max()} characters\")\n"
   ],
   "id": "2694b57b36fd417d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Audio Preprocessing\n",
    "\n",
    "Now, let's implement functions to load and preprocess the audio files. We'll convert the audio to spectrograms, which will be the input to our neural network.\n"
   ],
   "id": "b4bd5c3c4f1b26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to read a wav file and return the audio data\n",
    "def read_wav_file(wav_file):\n",
    "    with wave.open(wav_file, \"rb\") as wav:\n",
    "        # Get basic information about the wav file\n",
    "        n_channels = wav.getnchannels()\n",
    "        sample_width = wav.getsampwidth()\n",
    "        frame_rate = wav.getframerate()\n",
    "        n_frames = wav.getnframes()\n",
    "\n",
    "        # Read the audio data\n",
    "        audio_data = wav.readframes(n_frames)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        audio_data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "\n",
    "        # Normalize to [-1, 1]\n",
    "        audio_data = audio_data.astype(np.float32) / 32768.0\n",
    "\n",
    "    return audio_data, frame_rate\n"
   ],
   "id": "cc19ac239cdc1c8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to compute spectrogram from audio data\n",
    "def compute_spectrogram(audio_data, frame_rate, frame_length=256, frame_step=160, fft_length=384):\n",
    "    # Compute the Short-Time Fourier Transform (STFT)\n",
    "    stft = tf.signal.stft(\n",
    "        audio_data,\n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        fft_length=fft_length,\n",
    "        window_fn=tf.signal.hann_window\n",
    "    )\n",
    "\n",
    "    # Compute the magnitude spectrogram\n",
    "    spectrogram = tf.abs(stft)\n",
    "\n",
    "    # Apply the mel filterbank\n",
    "    num_mel_bins = 80\n",
    "    lower_edge_hertz = 80.0\n",
    "    upper_edge_hertz = 7600.0\n",
    "\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins, spectrogram.shape[-1], frame_rate, lower_edge_hertz, upper_edge_hertz\n",
    "    )\n",
    "\n",
    "    mel_spectrogram = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\n",
    "\n",
    "    # Convert to log scale (dB)\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
    "\n",
    "    return log_mel_spectrogram\n"
   ],
   "id": "196114cbbf58ede7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's visualize a spectrogram for one audio file\n",
    "sample_wav_file = metadata_df[\"wav_file\"].iloc[0]\n",
    "sample_audio_data, sample_frame_rate = read_wav_file(sample_wav_file)\n",
    "sample_spectrogram = compute_spectrogram(sample_audio_data, sample_frame_rate)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(tf.transpose(sample_spectrogram), aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Log Mel Spectrogram\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Play the audio\n",
    "Audio(sample_audio_data, rate=sample_frame_rate)\n"
   ],
   "id": "429c141982de07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "Next, let's preprocess the text data. We'll create a character-level tokenizer to convert text to sequences of integers.\n"
   ],
   "id": "73bc3150bf5ae371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a vocabulary of characters\n",
    "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "print(f\"The vocabulary is: {char_to_num.get_vocabulary()} (size: {len(char_to_num.get_vocabulary())})\")\n"
   ],
   "id": "79418d6de7c9f43b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters except those in our vocabulary\n",
    "    text = \"\".join([c for c in text if c in characters])\n",
    "\n",
    "    # Convert to sequence of integers\n",
    "    text_encoded = char_to_num(tf.strings.unicode_split(text, input_encoding=\"UTF-8\"))\n",
    "\n",
    "    return text_encoded\n"
   ],
   "id": "be82469e16f72169",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test the text preprocessing\n",
    "sample_text = metadata_df[\"transcription\"].iloc[0]\n",
    "print(f\"Original text: {sample_text}\")\n",
    "processed_text = preprocess_text(sample_text)\n",
    "print(f\"Processed text: {processed_text}\")\n",
    "decoded_text = tf.strings.reduce_join(num_to_char(processed_text)).numpy().decode(\"utf-8\")\n",
    "print(f\"Decoded text: {decoded_text}\")\n"
   ],
   "id": "5cfbb89786203dbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Data Generator\n",
    "\n",
    "Now, let's create a data generator to load and preprocess the data in batches.\n"
   ],
   "id": "2f373eb47cff5140"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(metadata_df, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n"
   ],
   "id": "17ff901c63802db8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calc_output_length(input_length: int):  # length calculate helper\n",
    "    # Example for 2 conv layers with stride 2 each (adjust based on your architecture)\n",
    "    length = input_length\n",
    "    length = length // 2  # after first conv/pool\n",
    "    length = length // 2  # after second conv/pool\n",
    "    return length\n",
    "\n",
    "\n",
    "# Create a data generator\n",
    "class AudioDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, batch_size=32, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(dataframe))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_df = self.dataframe.iloc[batch_indices]\n",
    "\n",
    "        batch_spectrograms = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            # Load and preprocess audio\n",
    "            audio_data, frame_rate = read_wav_file(row[\"wav_file\"])\n",
    "            spectrogram = compute_spectrogram(audio_data, frame_rate)\n",
    "\n",
    "            # Preprocess text\n",
    "            text = row[\"transcription\"]\n",
    "            text_encoded = preprocess_text(text)\n",
    "            if spectrogram.shape[0] < 2 * len(text_encoded) - 1:\n",
    "                continue\n",
    "\n",
    "            # Skip empty inputs or labels\n",
    "            if spectrogram.shape[0] == 0:\n",
    "                print(f\"Skipping sample with zero-length spectrogram: {row['wav_file']}\")\n",
    "                continue\n",
    "            if len(text_encoded) == 0:\n",
    "                print(f\"Skipping sample with empty label: {row['wav_file']} transcription: {text}\")\n",
    "                continue\n",
    "\n",
    "            batch_spectrograms.append(spectrogram)\n",
    "            batch_labels.append(text_encoded)\n",
    "\n",
    "        if len(batch_spectrograms) == 0:\n",
    "            raise ValueError(\"No valid samples in batch\")\n",
    "\n",
    "        # Pad spectrograms\n",
    "        max_spectrogram_length = max([s.shape[0] for s in batch_spectrograms])\n",
    "        padded_spectrograms = []\n",
    "        for spectrogram in batch_spectrograms:\n",
    "            padded_spectrogram = tf.pad(\n",
    "                spectrogram,\n",
    "                [[0, max_spectrogram_length - spectrogram.shape[0]], [0, 0]],\n",
    "                \"CONSTANT\"\n",
    "            )\n",
    "            padded_spectrograms.append(padded_spectrogram)\n",
    "\n",
    "        batch_spectrograms = tf.stack(padded_spectrograms)\n",
    "\n",
    "        # Pad labels\n",
    "        batch_labels = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            batch_labels, padding=\"post\"\n",
    "        )\n",
    "\n",
    "        input_lengths = tf.expand_dims(\n",
    "            tf.ones(batch_spectrograms.shape[0], dtype=tf.int32) * calc_output_length(max_spectrogram_length),\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        label_lengths = tf.expand_dims(\n",
    "            tf.reduce_sum(tf.cast(batch_labels != 0, tf.int32), axis=1),\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            \"input\": batch_spectrograms,\n",
    "            \"input_length\": input_lengths,\n",
    "            \"label_length\": label_lengths,\n",
    "            \"label\": batch_labels\n",
    "        }\n",
    "        outputs = tf.zeros((batch_spectrograms.shape[0], 1), dtype=tf.float32)\n",
    "\n",
    "        return inputs, outputs\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ],
   "id": "2d12ee22e0824351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create data generators\n",
    "batch_size = 32\n",
    "train_generator = AudioDataGenerator(train_df, batch_size=batch_size)\n",
    "val_generator = AudioDataGenerator(val_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test the data generator\n",
    "inputs, outputs = train_generator[0]"
   ],
   "id": "d09038f125c2d6dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. CNN-bi-LSTM Model\n",
    "\n",
    "Now, let's implement the CNN-bi-LSTM model for speech recognition.\n"
   ],
   "id": "d14f949088c4e4a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the CTC loss function\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    batch_len = tf.shape(y_true)[0]\n",
    "    input_length = tf.shape(y_pred)[1]\n",
    "    label_length = tf.shape(y_true)[1]\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=tf.int32)\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=tf.int32)\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return tf.reduce_mean(loss)"
   ],
   "id": "d7f0762138e96b1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a custom layer to get tensor shape\n",
    "class ShapeLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ShapeLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Return the shape as a concrete tensor, not a symbolic one\n",
    "        return tf.shape(inputs)\n",
    "\n",
    "\n",
    "# Define a custom layer for dynamic reshaping\n",
    "class DynamicReshapeLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DynamicReshapeLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get the shape information from the inputs\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        # For a 4D tensor from Conv2D, the shape is [batch, time, height, width, channels]\n",
    "        # or for some cases it might be [batch, time, height, width]\n",
    "        shape = tf.shape(inputs)\n",
    "\n",
    "        # Handle both 4D and 5D tensors\n",
    "        if len(inputs.shape) == 4:  # [batch, time, height, width]\n",
    "            time_steps = shape[1]\n",
    "            height = shape[2]\n",
    "            width = shape[3]\n",
    "            channels = 1\n",
    "        else:  # [batch, time, height, width, channels]\n",
    "            time_steps = shape[1]\n",
    "            height = shape[2]\n",
    "            width = shape[3]\n",
    "            channels = shape[4]\n",
    "\n",
    "        # Reshape to flatten height, width, and channels while preserving time dimension\n",
    "        return tf.reshape(inputs, [batch_size, time_steps, height * width * channels])\n",
    "\n",
    "\n",
    "# Define the CNN-bi-LSTM model\n",
    "def build_model(input_dim, output_dim):\n",
    "    # Input layer\n",
    "    input_spectrogram = layers.Input(shape=(None, input_dim), name=\"input\")\n",
    "    input_length = layers.Input(shape=(1,), dtype=tf.int32, name=\"input_length\")\n",
    "    label = layers.Input(shape=(None,), dtype=tf.int32, name=\"label\")\n",
    "    label_length = layers.Input(shape=(1,), dtype=tf.int32, name=\"label_length\")\n",
    "\n",
    "    # Expand dimensions for CNN\n",
    "    x = layers.Reshape((-1, input_dim, 1))(input_spectrogram)\n",
    "\n",
    "    # CNN layers\n",
    "    x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Reshape for RNN using the custom DynamicReshapeLayer\n",
    "    dynamic_reshape_layer = DynamicReshapeLayer()\n",
    "    x = dynamic_reshape_layer(x)\n",
    "\n",
    "    # Bidirectional LSTM layers\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(output_dim + 1, activation=\"softmax\")(x)\n",
    "\n",
    "    # Define the loss function\n",
    "    def ctc_loss_function(args):\n",
    "        y_pred, labels, input_length, label_length = args\n",
    "        return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "    # Add the CTC loss\n",
    "    ctc_output = layers.Lambda(ctc_loss_function, output_shape=(1,), name=\"ctc\")(\n",
    "        [x, label, input_length, label_length]\n",
    "    )\n",
    "    # Define the training model\n",
    "    training_model = tf.keras.Model(\n",
    "        inputs=[input_spectrogram, input_length, label, label_length],\n",
    "        outputs=ctc_output\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    training_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=lambda y_true, y_pred: y_pred)\n",
    "\n",
    "    # Define the prediction model\n",
    "    prediction_model = tf.keras.Model(inputs=input_spectrogram, outputs=x)\n",
    "\n",
    "    return training_model, prediction_model\n"
   ],
   "id": "e09d8dbfd1c1ddc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the model\n",
    "input_dim = 80  # Number of mel frequency bins\n",
    "output_dim = len(char_to_num.get_vocabulary())  # Number of characters in the vocabulary\n",
    "\n",
    "training_model, prediction_model = build_model(input_dim, output_dim)\n",
    "\n",
    "# Print the model summary\n",
    "training_model.summary()\n"
   ],
   "id": "f3b561e4367e4fc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Model Training\n",
    "\n",
    "Now, let's train the model.\n"
   ],
   "id": "aa41b9cb97168261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define callbacks\n",
    "checkpoint_path = \"models/cnn_bilstm_speech_recognition.weights.h5\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n"
   ],
   "id": "15916e0091f9a9fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "# epochs = 50\n",
    "epochs = 1\n",
    "# try:\n",
    "#     training_model.load_weights(checkpoint_path)\n",
    "# except FileNotFoundError:\n",
    "#     pass\n",
    "\n",
    "history = training_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback],\n",
    ")\n"
   ],
   "id": "4f879c2c4fc2fd52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"lr\"])\n",
    "plt.title(\"Learning Rate\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "53d5476eae8b0a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Model Evaluation and Inference\n",
    "\n",
    "Now, let's evaluate the model and use it for inference.\n"
   ],
   "id": "39a4d5164b773bf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the best model weights\n",
    "prediction_model.load_weights(checkpoint_path)\n"
   ],
   "id": "6724661e86bc6c35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to decode the predictions\n",
    "def decode_predictions(pred):\n",
    "    # Use greedy decoding (best path)\n",
    "    pred = tf.argmax(pred, axis=-1)\n",
    "    pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "    # Convert to characters\n",
    "    pred = num_to_char(pred)\n",
    "\n",
    "    # Join characters to form words\n",
    "    pred = tf.strings.reduce_join(pred, axis=-1)\n",
    "\n",
    "    return pred.numpy()\n"
   ],
   "id": "ad86d971631001fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to perform CTC beam search decoding\n",
    "def decode_batch_predictions(pred):\n",
    "    # Use CTC beam search decoding\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=False, beam_width=10)[0][0]\n",
    "\n",
    "    # Convert to characters\n",
    "    output_text = []\n",
    "    for result in results:\n",
    "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(result)\n",
    "\n",
    "    return output_text\n"
   ],
   "id": "1a1c7f695d86f8c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test the model on a few samples\n",
    "test_samples = val_df.sample(5)\n",
    "\n",
    "for _, row in test_samples.iterrows():\n",
    "    # Load and preprocess audio\n",
    "    audio_data, frame_rate = read_wav_file(row[\"wav_file\"])\n",
    "    spectrogram = compute_spectrogram(audio_data, frame_rate)\n",
    "\n",
    "    # Expand dimensions for batch\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=0)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = prediction_model.predict(spectrogram)\n",
    "\n",
    "    # Decode prediction\n",
    "    decoded_prediction = decode_batch_predictions(prediction)[0]\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Original text: {row['transcription']}\")\n",
    "    print(f\"Predicted text: {decoded_prediction}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Play the audio\n",
    "    display(Audio(audio_data, rate=frame_rate))\n"
   ],
   "id": "bcee475401dde118",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
